#!/usr/bin/env python3
"""
Comment Trend Analyzer - ëŒ“ê¸€ ì¤‘ì‹¬ ìŒì•… íŠ¸ë Œë“œ ë¶„ì„
ê°ì •ë¶„ì„, í† í”½ëª¨ë¸ë§, í‚¤ì›Œë“œ ì¶”ì¶œ, MCP í†µí•©
"""

import os
import re
import json
import time
import math
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Set
from collections import Counter, defaultdict
import statistics

SKLEARN_AVAILABLE = False

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False

try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except ImportError:
    VADER_AVAILABLE = False

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

class CommentTrendAnalyzer:
    def __init__(self, console_log=None):
        """
        ëŒ“ê¸€ íŠ¸ë Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            console_log: ë¡œê·¸ ì¶œë ¥ í•¨ìˆ˜
        """
        self.console_log = console_log or print
        
        # ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.vader_analyzer = None
        if VADER_AVAILABLE:
            try:
                self.vader_analyzer = SentimentIntensityAnalyzer()
                self.console_log("[Comment] VADER ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.console_log(f"[Comment] VADER ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = None
        if OPENAI_AVAILABLE:
            try:
                api_key = os.getenv('OPENAI_API_KEY')
                if api_key:
                    self.openai_client = OpenAI(api_key=api_key)
                    self.console_log("[Comment] OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    self.console_log("[Comment] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            except Exception as e:
                self.console_log(f"[Comment] OpenAI ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        
        # í† í”½ ëª¨ë¸ë§ ë¹„í™œì„±í™”
        self.topic_model = None
        self.vectorizer = None
        
        # ìŒì•… ê´€ë ¨ ê°ì • í‚¤ì›Œë“œ í™•ì¥
        self.music_sentiment_keywords = {
            'positive_intense': {
                'words': ['ëŒ€ë°•', 'ë¯¸ì³¤ë‹¤', 'ì§±', 'ìµœê³ ', 'ì™„ë²½', 'ë ˆì „ë“œ', 'ëª…ê³¡', 'ê°“ê³¡', 'masterpiece', 'amazing', 'incredible'],
                'weight': 2.0
            },
            'positive_mild': {
                'words': ['ì¢‹ë‹¤', 'ê´œì°®ë‹¤', 'ë§ˆìŒì—ë“¤ì–´', 'ë“£ê¸°ì¢‹ë‹¤', 'good', 'nice', 'love', 'like'],
                'weight': 1.0
            },
            'negative_intense': {
                'words': ['ìµœì•…', 'ë³„ë¡œ', 'ì§œì¦', 'ì‹¤ë§', 'ë§í–ˆë‹¤', 'terrible', 'awful', 'hate', 'worst'],
                'weight': -2.0
            },
            'negative_mild': {
                'words': ['ì•„ì‰½ë‹¤', 'ê·¸ì €ê·¸ë ‡ë‹¤', 'ë³„ë¡œì•¼', 'okay', 'meh', 'not bad'],
                'weight': -1.0
            },
            'excitement': {
                'words': ['ì‹ ë‚œë‹¤', 'í¥ê²¹ë‹¤', 'ì¶¤ì¶”ê³ ì‹¶ì–´', 'ì—ë„ˆì§€', 'energetic', 'exciting', 'pump', 'hype'],
                'weight': 1.5
            },
            'calm': {
                'words': ['í¸ì•ˆí•˜ë‹¤', 'ì”ì”í•˜ë‹¤', 'íë§', 'ì°¨ë¶„í•˜ë‹¤', 'chill', 'relaxing', 'peaceful', 'soothing'],
                'weight': 0.8
            }
        }
        
        # ìŒì•… ì¥ë¥´ ê´€ë ¨ í‚¤ì›Œë“œ
        self.genre_indicators = {
            'kpop': ['ì•„ì´ëŒ', 'idol', 'ì¼€ì´íŒ', 'k-pop', 'í•œêµ­', 'korean', 'ê·¸ë£¹'],
            'hiphop': ['ë©', 'rap', 'í™í•©', 'hip-hop', 'ë¹„íŠ¸', 'beat', 'flow', 'ë¼ì´ë°'],
            'ballad': ['ë°œë¼ë“œ', 'ballad', 'ê°ì„±', 'ìŠ¬í”„ë‹¤', 'emotional', 'sad'],
            'dance': ['ëŒ„ìŠ¤', 'dance', 'ì‹ ë‚˜ë‹¤', 'í´ëŸ½', 'club', 'party'],
            'rock': ['ë¡', 'rock', 'ê¸°íƒ€', 'guitar', 'ë°´ë“œ', 'band'],
            'pop': ['íŒ', 'pop', 'ë©œë¡œë””', 'melody', 'catchy']
        }
        
        # íŠ¸ë Œë“œ ì§€í‘œ í‚¤ì›Œë“œ
        self.trend_indicators = {
            'viral': ['ë°”ì´ëŸ´', 'viral', 'ìœ í–‰', 'trending', 'ì¸ê¸°í­ë°œ'],
            'chart': ['ì°¨íŠ¸', 'chart', 'ìˆœìœ„', '1ìœ„', 'number one', 'top'],
            'new_release': ['ì‹ ê³¡', 'new song', 'ìƒˆë¡œë‚˜ì˜¨', 'latest', 'just released'],
            'comeback': ['ì»´ë°±', 'comeback', 'ëŒì•„ì™”ë‹¤', 'return'],
            'collaboration': ['ì½œë¼ë³´', 'collab', 'í”¼ì²˜ë§', 'feat', 'featuring']
        }
    
    def analyze_comment_sentiment(self, comments: List[Dict]) -> Dict:
        """
        ëŒ“ê¸€ ê°ì • ë¶„ì„
        
        Args:
            comments: ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ [{'text': str, 'timestamp': str, 'source': str}]
            
        Returns:
            ê°ì • ë¶„ì„ ê²°ê³¼
        """
        try:
            if not comments:
                return {'error': 'ë¶„ì„í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤'}
            
            sentiment_results = []
            emotion_distribution = defaultdict(int)
            platform_sentiment = defaultdict(list)
            
            for comment in comments:
                text = comment.get('text', '')
                source = comment.get('source', 'unknown')
                
                if not text.strip():
                    continue
                
                # ë‹¤ì¤‘ ê°ì • ë¶„ì„
                sentiment_scores = {}
                
                # VADER ê°ì • ë¶„ì„ (ì˜ì–´ ì¤‘ì‹¬)
                if self.vader_analyzer:
                    vader_scores = self.vader_analyzer.polarity_scores(text)
                    sentiment_scores['vader'] = {
                        'compound': vader_scores['compound'],
                        'positive': vader_scores['pos'],
                        'negative': vader_scores['neg'],
                        'neutral': vader_scores['neu']
                    }
                
                # TextBlob ê°ì • ë¶„ì„
                if TEXTBLOB_AVAILABLE:
                    try:
                        blob = TextBlob(text)
                        sentiment_scores['textblob'] = {
                            'polarity': blob.sentiment.polarity,
                            'subjectivity': blob.sentiment.subjectivity
                        }
                    except:
                        pass
                
                # ì»¤ìŠ¤í…€ ìŒì•… ê°ì • ë¶„ì„
                music_sentiment = self._analyze_music_sentiment(text)
                sentiment_scores['music_custom'] = music_sentiment
                
                # ì¢…í•© ê°ì • ì ìˆ˜ ê³„ì‚°
                final_sentiment = self._calculate_combined_sentiment(sentiment_scores)
                
                # ê°ì • ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                emotion_category = self._categorize_emotion(final_sentiment)
                emotion_distribution[emotion_category] += 1
                
                # í”Œë«í¼ë³„ ê°ì • ì €ì¥
                platform_sentiment[source].append(final_sentiment['score'])
                
                sentiment_results.append({
                    'text': text[:100] + '...' if len(text) > 100 else text,
                    'sentiment_scores': sentiment_scores,
                    'final_sentiment': final_sentiment,
                    'emotion_category': emotion_category,
                    'source': source,
                    'timestamp': comment.get('timestamp')
                })
            
            # ì „ì²´ í†µê³„ ê³„ì‚°
            all_scores = [result['final_sentiment']['score'] for result in sentiment_results]
            
            # í”Œë«í¼ë³„ í‰ê·  ê°ì •
            platform_averages = {}
            for platform, scores in platform_sentiment.items():
                platform_averages[platform] = {
                    'average': statistics.mean(scores) if scores else 0,
                    'count': len(scores)
                }
            
            result = {
                'total_comments_analyzed': len(sentiment_results),
                'overall_sentiment': {
                    'average_score': statistics.mean(all_scores) if all_scores else 0,
                    'median_score': statistics.median(all_scores) if all_scores else 0,
                    'std_deviation': statistics.stdev(all_scores) if len(all_scores) > 1 else 0
                },
                'emotion_distribution': dict(emotion_distribution),
                'platform_sentiment': platform_averages,
                'detailed_results': sentiment_results[:20],  # ìƒìœ„ 20ê°œë§Œ ì €ì¥
                'sentiment_trend': self._calculate_sentiment_trend(sentiment_results),
                'analyzed_at': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            self.console_log(f"[Comment] ëŒ“ê¸€ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_music_sentiment(self, text: str) -> Dict:
        """ìŒì•… ê´€ë ¨ ì»¤ìŠ¤í…€ ê°ì • ë¶„ì„"""
        text_lower = text.lower()
        sentiment_score = 0
        matched_categories = []
        
        for category, data in self.music_sentiment_keywords.items():
            matches = 0
            for keyword in data['words']:
                if keyword.lower() in text_lower:
                    matches += text_lower.count(keyword.lower())
            
            if matches > 0:
                sentiment_score += matches * data['weight']
                matched_categories.append(category)
        
        # ì •ê·œí™” (-1 ~ 1)
        normalized_score = max(-1, min(1, sentiment_score / 5))
        
        return {
            'score': normalized_score,
            'matched_categories': matched_categories,
            'confidence': min(1.0, abs(sentiment_score) / 2)
        }
    
    def _calculate_combined_sentiment(self, sentiment_scores: Dict) -> Dict:
        """ì—¬ëŸ¬ ê°ì • ë¶„ì„ ê²°ê³¼ ì¢…í•©"""
        scores = []
        weights = {'vader': 0.3, 'textblob': 0.3, 'music_custom': 0.4}
        
        final_score = 0
        total_weight = 0
        
        # VADER ì ìˆ˜
        if 'vader' in sentiment_scores:
            vader_score = sentiment_scores['vader']['compound']
            final_score += vader_score * weights['vader']
            total_weight += weights['vader']
        
        # TextBlob ì ìˆ˜
        if 'textblob' in sentiment_scores:
            textblob_score = sentiment_scores['textblob']['polarity']
            final_score += textblob_score * weights['textblob']
            total_weight += weights['textblob']
        
        # ìŒì•… ì»¤ìŠ¤í…€ ì ìˆ˜
        if 'music_custom' in sentiment_scores:
            music_score = sentiment_scores['music_custom']['score']
            final_score += music_score * weights['music_custom']
            total_weight += weights['music_custom']
        
        # ê°€ì¤‘ í‰ê· 
        if total_weight > 0:
            final_score = final_score / total_weight
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = total_weight / sum(weights.values())
        
        return {
            'score': final_score,
            'confidence': confidence,
            'label': 'positive' if final_score > 0.1 else 'negative' if final_score < -0.1 else 'neutral'
        }
    
    def _categorize_emotion(self, sentiment: Dict) -> str:
        """ê°ì •ì„ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        score = sentiment['score']
        
        if score > 0.5:
            return 'very_positive'
        elif score > 0.1:
            return 'positive'
        elif score > -0.1:
            return 'neutral'
        elif score > -0.5:
            return 'negative'
        else:
            return 'very_negative'
    
    def _calculate_sentiment_trend(self, sentiment_results: List[Dict]) -> Dict:
        """ê°ì • ë³€í™” íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(sentiment_results) < 5:
            return {'trend': 'insufficient_data'}
        
        # ì‹œê°„ìˆœ ì •ë ¬ (timestampê°€ ìˆëŠ” ê²½ìš°)
        sorted_results = sorted(
            [r for r in sentiment_results if r.get('timestamp')],
            key=lambda x: x['timestamp']
        )
        
        if len(sorted_results) < 5:
            # timestampê°€ ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ ë¶„ì„
            sorted_results = sentiment_results
        
        # ì´ˆê¸° vs ìµœê·¼ ë¹„êµ
        first_half = sorted_results[:len(sorted_results)//2]
        second_half = sorted_results[len(sorted_results)//2:]
        
        first_avg = statistics.mean([r['final_sentiment']['score'] for r in first_half])
        second_avg = statistics.mean([r['final_sentiment']['score'] for r in second_half])
        
        change = second_avg - first_avg
        
        if change > 0.1:
            trend = 'improving'
        elif change < -0.1:
            trend = 'declining'
        else:
            trend = 'stable'
        
        return {
            'trend': trend,
            'change_magnitude': change,
            'early_sentiment': first_avg,
            'recent_sentiment': second_avg
        }
    
    def extract_comment_topics(self, comments: List[str], num_topics: int = 5) -> Dict:
        """
        ëŒ“ê¸€ì—ì„œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í† í”½ ë¶„ì„
        
        Args:
            comments: ëŒ“ê¸€ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            num_topics: ì¶”ì¶œí•  í† í”½ ìˆ˜ (ë¬´ì‹œë¨)
            
        Returns:
            í‚¤ì›Œë“œ ê¸°ë°˜ í† í”½ ë¶„ì„ ê²°ê³¼
        """
        try:
            if len(comments) < 5:
                return {'error': 'í† í”½ ë¶„ì„ì„ ìœ„í•´ ìµœì†Œ 5ê°œ ì´ìƒì˜ ëŒ“ê¸€ì´ í•„ìš”í•©ë‹ˆë‹¤'}
            
            # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
            all_keywords = []
            for comment in comments:
                # ê¸°ë³¸ ì „ì²˜ë¦¬
                clean_text = re.sub(r'[^\w\sê°€-í£]', ' ', comment.lower())
                words = clean_text.split()
                
                # 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ í¬í•¨
                filtered_words = [word for word in words if len(word) >= 2]
                all_keywords.extend(filtered_words)
            
            # ë¹ˆë„ ê³„ì‚°
            keyword_counts = Counter(all_keywords)
            top_keywords = keyword_counts.most_common(20)
            
            # ìŒì•… ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ë¥˜
            music_topics = self._categorize_keywords_by_music_type([kw[0] for kw in top_keywords])
            
            result = {
                'analysis_type': 'keyword_based',
                'top_keywords': dict(top_keywords),
                'music_topics': music_topics,
                'total_comments_analyzed': len(comments),
                'total_keywords_found': len(keyword_counts),
                'analyzed_at': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            self.console_log(f"[Comment] í‚¤ì›Œë“œ í† í”½ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e)}
    
    def _categorize_keywords_by_music_type(self, keywords: List[str]) -> Dict:
        """í‚¤ì›Œë“œë¥¼ ìŒì•… ìœ í˜•ë³„ë¡œ ë¶„ë¥˜"""
        categorized = {'genre': [], 'trend': [], 'sentiment': [], 'other': []}
        
        for keyword in keywords:
            categorized_flag = False
            
            # ì¥ë¥´ í‚¤ì›Œë“œ ê²€ì‚¬
            for genre, indicators in self.genre_indicators.items():
                if any(indicator.lower() in keyword.lower() for indicator in indicators):
                    categorized['genre'].append({'keyword': keyword, 'category': genre})
                    categorized_flag = True
                    break
            
            if not categorized_flag:
                # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê²€ì‚¬
                for trend_type, indicators in self.trend_indicators.items():
                    if any(indicator.lower() in keyword.lower() for indicator in indicators):
                        categorized['trend'].append({'keyword': keyword, 'category': trend_type})
                        categorized_flag = True
                        break
            
            if not categorized_flag:
                # ê°ì • í‚¤ì›Œë“œ ê²€ì‚¬
                for sentiment_type, data in self.music_sentiment_keywords.items():
                    if any(word.lower() in keyword.lower() for word in data['words']):
                        categorized['sentiment'].append({'keyword': keyword, 'category': sentiment_type})
                        categorized_flag = True
                        break
            
            if not categorized_flag:
                categorized['other'].append(keyword)
        
        return categorized
    
    
    def analyze_comment_patterns(self, comments: List[Dict]) -> Dict:
        """
        ëŒ“ê¸€ íŒ¨í„´ ë¶„ì„
        
        Args:
            comments: ëŒ“ê¸€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ëŒ“ê¸€ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        try:
            if not comments:
                return {'error': 'ë¶„ì„í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤'}
            
            # ê¸°ë³¸ í†µê³„
            comment_lengths = [len(comment.get('text', '')) for comment in comments]
            word_counts = [len(comment.get('text', '').split()) for comment in comments]
            
            # ì‹œê°„ íŒ¨í„´ ë¶„ì„
            time_patterns = self._analyze_time_patterns(comments)
            
            # ì–¸ì–´ íŒ¨í„´ ë¶„ì„
            language_patterns = self._analyze_language_patterns(comments)
            
            # ì°¸ì—¬ë„ íŒ¨í„´ ë¶„ì„
            engagement_patterns = self._analyze_engagement_patterns(comments)
            
            # ì´ëª¨ì§€/ì´ëª¨í‹°ì½˜ ë¶„ì„
            emoji_patterns = self._analyze_emoji_patterns(comments)
            
            result = {
                'basic_statistics': {
                    'total_comments': len(comments),
                    'avg_comment_length': statistics.mean(comment_lengths) if comment_lengths else 0,
                    'avg_word_count': statistics.mean(word_counts) if word_counts else 0,
                    'max_comment_length': max(comment_lengths) if comment_lengths else 0,
                    'min_comment_length': min(comment_lengths) if comment_lengths else 0
                },
                'time_patterns': time_patterns,
                'language_patterns': language_patterns,
                'engagement_patterns': engagement_patterns,
                'emoji_patterns': emoji_patterns,
                'analyzed_at': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            self.console_log(f"[Comment] ëŒ“ê¸€ íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_time_patterns(self, comments: List[Dict]) -> Dict:
        """ëŒ“ê¸€ ì‹œê°„ íŒ¨í„´ ë¶„ì„"""
        timestamps = []
        for comment in comments:
            timestamp = comment.get('timestamp')
            if timestamp:
                try:
                    if isinstance(timestamp, str):
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    else:
                        dt = timestamp
                    timestamps.append(dt)
                except:
                    continue
        
        if not timestamps:
            return {'error': 'ì‹œê°„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # ì‹œê°„ëŒ€ë³„ ë¶„í¬
        hour_distribution = defaultdict(int)
        day_distribution = defaultdict(int)
        
        for ts in timestamps:
            hour_distribution[ts.hour] += 1
            day_distribution[ts.strftime('%A')] += 1
        
        # í”¼í¬ ì‹œê°„ ì°¾ê¸°
        peak_hour = max(hour_distribution.items(), key=lambda x: x[1])[0] if hour_distribution else None
        peak_day = max(day_distribution.items(), key=lambda x: x[1])[0] if day_distribution else None
        
        return {
            'hour_distribution': dict(hour_distribution),
            'day_distribution': dict(day_distribution),
            'peak_hour': peak_hour,
            'peak_day': peak_day,
            'total_timespan_hours': (max(timestamps) - min(timestamps)).total_seconds() / 3600 if len(timestamps) > 1 else 0
        }
    
    def _analyze_language_patterns(self, comments: List[Dict]) -> Dict:
        """ì–¸ì–´ íŒ¨í„´ ë¶„ì„"""
        korean_count = 0
        english_count = 0
        mixed_count = 0
        emoji_count = 0
        
        for comment in comments:
            text = comment.get('text', '')
            
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            emoji_chars = len(re.findall(r'[ğŸ˜€-ğŸ™]', text))
            
            if emoji_chars > 0:
                emoji_count += 1
            
            if korean_chars > english_chars * 2:
                korean_count += 1
            elif english_chars > korean_chars * 2:
                english_count += 1
            else:
                mixed_count += 1
        
        total = len(comments)
        return {
            'korean_dominant': korean_count,
            'english_dominant': english_count,
            'mixed_language': mixed_count,
            'emoji_usage': emoji_count,
            'korean_percentage': (korean_count / total * 100) if total > 0 else 0,
            'english_percentage': (english_count / total * 100) if total > 0 else 0,
            'emoji_percentage': (emoji_count / total * 100) if total > 0 else 0
        }
    
    def _analyze_engagement_patterns(self, comments: List[Dict]) -> Dict:
        """ì°¸ì—¬ë„ íŒ¨í„´ ë¶„ì„"""
        scores = [comment.get('score', 0) for comment in comments if 'score' in comment]
        reply_counts = [comment.get('reply_count', 0) for comment in comments if 'reply_count' in comment]
        
        if not scores and not reply_counts:
            return {'error': 'ì°¸ì—¬ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        result = {}
        
        if scores:
            result['score_stats'] = {
                'avg_score': statistics.mean(scores),
                'max_score': max(scores),
                'min_score': min(scores),
                'high_score_comments': len([s for s in scores if s > 10])
            }
        
        if reply_counts:
            result['reply_stats'] = {
                'avg_replies': statistics.mean(reply_counts),
                'max_replies': max(reply_counts),
                'comments_with_replies': len([r for r in reply_counts if r > 0])
            }
        
        return result
    
    def _analyze_emoji_patterns(self, comments: List[Dict]) -> Dict:
        """ì´ëª¨ì§€/ì´ëª¨í‹°ì½˜ íŒ¨í„´ ë¶„ì„"""
        emoji_counter = Counter()
        emoticon_counter = Counter()
        
        # ì´ëª¨í‹°ì½˜ íŒ¨í„´ (ê°„ë‹¨í•œ)
        emoticon_patterns = [
            r':-?\)', r':-?\(', r':-?D', r':-?P', r':-?o', r':-?\|',
            r':\)', r':\(', r':D', r':P', r':o', r':\|',
            r'\^_\^', r'T_T', r'ã… ã… ', r'ã…‹ã…‹', r'ã…ã…'
        ]
        
        for comment in comments:
            text = comment.get('text', '')
            
            # ì´ëª¨ì§€ ì¹´ìš´íŠ¸
            emojis = re.findall(r'[ğŸ˜€-ğŸ™]', text)
            for emoji in emojis:
                emoji_counter[emoji] += 1
            
            # ì´ëª¨í‹°ì½˜ ì¹´ìš´íŠ¸
            for pattern in emoticon_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    emoticon_counter[match] += 1
        
        return {
            'top_emojis': dict(emoji_counter.most_common(10)),
            'top_emoticons': dict(emoticon_counter.most_common(10)),
            'total_emoji_usage': sum(emoji_counter.values()),
            'total_emoticon_usage': sum(emoticon_counter.values()),
            'emoji_diversity': len(emoji_counter),
            'emoticon_diversity': len(emoticon_counter)
        }
    
    def get_analysis_status(self) -> Dict:
        """ë¶„ì„ê¸° ìƒíƒœ í™•ì¸"""
        return {
            'sklearn_available': False,
            'textblob_available': TEXTBLOB_AVAILABLE,
            'vader_available': VADER_AVAILABLE,
            'vader_initialized': self.vader_analyzer is not None,
            'vectorizer_ready': self.vectorizer is not None,
            'sentiment_categories': len(self.music_sentiment_keywords),
            'genre_indicators': len(self.genre_indicators),
            'trend_indicators': len(self.trend_indicators)
        }