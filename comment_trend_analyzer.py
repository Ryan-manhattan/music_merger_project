#!/usr/bin/env python3
"""
Comment Trend Analyzer - ëŒ“ê¸€ ì¤‘ì‹¬ ìŒì•… íŠ¸ë Œë“œ ë¶„ì„
ê°ì •ë¶„ì„, í† í”½ëª¨ë¸ë§, í‚¤ì›Œë“œ ì¶”ì¶œ, MCP í†µí•©
"""

import os
import re
import json
import time
import math
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Set
from collections import Counter, defaultdict
import statistics

try:
    import numpy as np
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.decomposition import LatentDirichletAllocation
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False

try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except ImportError:
    VADER_AVAILABLE = False

class CommentTrendAnalyzer:
    def __init__(self, console_log=None):
        """
        ëŒ“ê¸€ íŠ¸ë Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            console_log: ë¡œê·¸ ì¶œë ¥ í•¨ìˆ˜
        """
        self.console_log = console_log or print
        
        # ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.vader_analyzer = None
        if VADER_AVAILABLE:
            try:
                self.vader_analyzer = SentimentIntensityAnalyzer()
                self.console_log("[Comment] VADER ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.console_log(f"[Comment] VADER ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        
        # í† í”½ ëª¨ë¸ë§ ì„¤ì •
        self.topic_model = None
        self.vectorizer = None
        
        if SKLEARN_AVAILABLE:
            self.vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.8
            )
        
        # ìŒì•… ê´€ë ¨ ê°ì • í‚¤ì›Œë“œ í™•ì¥
        self.music_sentiment_keywords = {
            'positive_intense': {
                'words': ['ëŒ€ë°•', 'ë¯¸ì³¤ë‹¤', 'ì§±', 'ìµœê³ ', 'ì™„ë²½', 'ë ˆì „ë“œ', 'ëª…ê³¡', 'ê°“ê³¡', 'masterpiece', 'amazing', 'incredible'],
                'weight': 2.0
            },
            'positive_mild': {
                'words': ['ì¢‹ë‹¤', 'ê´œì°®ë‹¤', 'ë§ˆìŒì—ë“¤ì–´', 'ë“£ê¸°ì¢‹ë‹¤', 'good', 'nice', 'love', 'like'],
                'weight': 1.0
            },
            'negative_intense': {
                'words': ['ìµœì•…', 'ë³„ë¡œ', 'ì§œì¦', 'ì‹¤ë§', 'ë§í–ˆë‹¤', 'terrible', 'awful', 'hate', 'worst'],
                'weight': -2.0
            },
            'negative_mild': {
                'words': ['ì•„ì‰½ë‹¤', 'ê·¸ì €ê·¸ë ‡ë‹¤', 'ë³„ë¡œì•¼', 'okay', 'meh', 'not bad'],
                'weight': -1.0
            },
            'excitement': {
                'words': ['ì‹ ë‚œë‹¤', 'í¥ê²¹ë‹¤', 'ì¶¤ì¶”ê³ ì‹¶ì–´', 'ì—ë„ˆì§€', 'energetic', 'exciting', 'pump', 'hype'],
                'weight': 1.5
            },
            'calm': {
                'words': ['í¸ì•ˆí•˜ë‹¤', 'ì”ì”í•˜ë‹¤', 'íë§', 'ì°¨ë¶„í•˜ë‹¤', 'chill', 'relaxing', 'peaceful', 'soothing'],
                'weight': 0.8
            }
        }
        
        # ìŒì•… ì¥ë¥´ ê´€ë ¨ í‚¤ì›Œë“œ
        self.genre_indicators = {
            'kpop': ['ì•„ì´ëŒ', 'idol', 'ì¼€ì´íŒ', 'k-pop', 'í•œêµ­', 'korean', 'ê·¸ë£¹'],
            'hiphop': ['ë©', 'rap', 'í™í•©', 'hip-hop', 'ë¹„íŠ¸', 'beat', 'flow', 'ë¼ì´ë°'],
            'ballad': ['ë°œë¼ë“œ', 'ballad', 'ê°ì„±', 'ìŠ¬í”„ë‹¤', 'emotional', 'sad'],
            'dance': ['ëŒ„ìŠ¤', 'dance', 'ì‹ ë‚˜ë‹¤', 'í´ëŸ½', 'club', 'party'],
            'rock': ['ë¡', 'rock', 'ê¸°íƒ€', 'guitar', 'ë°´ë“œ', 'band'],
            'pop': ['íŒ', 'pop', 'ë©œë¡œë””', 'melody', 'catchy']
        }
        
        # íŠ¸ë Œë“œ ì§€í‘œ í‚¤ì›Œë“œ
        self.trend_indicators = {
            'viral': ['ë°”ì´ëŸ´', 'viral', 'ìœ í–‰', 'trending', 'ì¸ê¸°í­ë°œ'],
            'chart': ['ì°¨íŠ¸', 'chart', 'ìˆœìœ„', '1ìœ„', 'number one', 'top'],
            'new_release': ['ì‹ ê³¡', 'new song', 'ìƒˆë¡œë‚˜ì˜¨', 'latest', 'just released'],
            'comeback': ['ì»´ë°±', 'comeback', 'ëŒì•„ì™”ë‹¤', 'return'],
            'collaboration': ['ì½œë¼ë³´', 'collab', 'í”¼ì²˜ë§', 'feat', 'featuring']
        }
    
    def analyze_comment_sentiment(self, comments: List[Dict]) -> Dict:
        """
        ëŒ“ê¸€ ê°ì • ë¶„ì„
        
        Args:
            comments: ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ [{'text': str, 'timestamp': str, 'source': str}]
            
        Returns:
            ê°ì • ë¶„ì„ ê²°ê³¼
        """
        try:
            if not comments:
                return {'error': 'ë¶„ì„í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤'}
            
            sentiment_results = []
            emotion_distribution = defaultdict(int)
            platform_sentiment = defaultdict(list)
            
            for comment in comments:
                text = comment.get('text', '')
                source = comment.get('source', 'unknown')
                
                if not text.strip():
                    continue
                
                # ë‹¤ì¤‘ ê°ì • ë¶„ì„
                sentiment_scores = {}
                
                # VADER ê°ì • ë¶„ì„ (ì˜ì–´ ì¤‘ì‹¬)
                if self.vader_analyzer:
                    vader_scores = self.vader_analyzer.polarity_scores(text)
                    sentiment_scores['vader'] = {
                        'compound': vader_scores['compound'],
                        'positive': vader_scores['pos'],
                        'negative': vader_scores['neg'],
                        'neutral': vader_scores['neu']
                    }
                
                # TextBlob ê°ì • ë¶„ì„
                if TEXTBLOB_AVAILABLE:
                    try:
                        blob = TextBlob(text)
                        sentiment_scores['textblob'] = {
                            'polarity': blob.sentiment.polarity,
                            'subjectivity': blob.sentiment.subjectivity
                        }
                    except:
                        pass
                
                # ì»¤ìŠ¤í…€ ìŒì•… ê°ì • ë¶„ì„
                music_sentiment = self._analyze_music_sentiment(text)
                sentiment_scores['music_custom'] = music_sentiment
                
                # ì¢…í•© ê°ì • ì ìˆ˜ ê³„ì‚°
                final_sentiment = self._calculate_combined_sentiment(sentiment_scores)
                
                # ê°ì • ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                emotion_category = self._categorize_emotion(final_sentiment)
                emotion_distribution[emotion_category] += 1
                
                # í”Œë«í¼ë³„ ê°ì • ì €ì¥
                platform_sentiment[source].append(final_sentiment['score'])
                
                sentiment_results.append({
                    'text': text[:100] + '...' if len(text) > 100 else text,
                    'sentiment_scores': sentiment_scores,
                    'final_sentiment': final_sentiment,
                    'emotion_category': emotion_category,
                    'source': source,
                    'timestamp': comment.get('timestamp')
                })
            
            # ì „ì²´ í†µê³„ ê³„ì‚°
            all_scores = [result['final_sentiment']['score'] for result in sentiment_results]
            
            # í”Œë«í¼ë³„ í‰ê·  ê°ì •
            platform_averages = {}
            for platform, scores in platform_sentiment.items():
                platform_averages[platform] = {
                    'average': statistics.mean(scores) if scores else 0,
                    'count': len(scores)
                }
            
            result = {
                'total_comments_analyzed': len(sentiment_results),
                'overall_sentiment': {
                    'average_score': statistics.mean(all_scores) if all_scores else 0,
                    'median_score': statistics.median(all_scores) if all_scores else 0,
                    'std_deviation': statistics.stdev(all_scores) if len(all_scores) > 1 else 0
                },
                'emotion_distribution': dict(emotion_distribution),
                'platform_sentiment': platform_averages,
                'detailed_results': sentiment_results[:20],  # ìƒìœ„ 20ê°œë§Œ ì €ì¥
                'sentiment_trend': self._calculate_sentiment_trend(sentiment_results),
                'analyzed_at': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            self.console_log(f"[Comment] ëŒ“ê¸€ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_music_sentiment(self, text: str) -> Dict:
        """ìŒì•… ê´€ë ¨ ì»¤ìŠ¤í…€ ê°ì • ë¶„ì„"""
        text_lower = text.lower()
        sentiment_score = 0
        matched_categories = []
        
        for category, data in self.music_sentiment_keywords.items():
            matches = 0
            for keyword in data['words']:
                if keyword.lower() in text_lower:
                    matches += text_lower.count(keyword.lower())
            
            if matches > 0:
                sentiment_score += matches * data['weight']
                matched_categories.append(category)
        
        # ì •ê·œí™” (-1 ~ 1)
        normalized_score = max(-1, min(1, sentiment_score / 5))
        
        return {
            'score': normalized_score,
            'matched_categories': matched_categories,
            'confidence': min(1.0, abs(sentiment_score) / 2)
        }
    
    def _calculate_combined_sentiment(self, sentiment_scores: Dict) -> Dict:
        """ì—¬ëŸ¬ ê°ì • ë¶„ì„ ê²°ê³¼ ì¢…í•©"""
        scores = []
        weights = {'vader': 0.3, 'textblob': 0.3, 'music_custom': 0.4}
        
        final_score = 0
        total_weight = 0
        
        # VADER ì ìˆ˜
        if 'vader' in sentiment_scores:
            vader_score = sentiment_scores['vader']['compound']
            final_score += vader_score * weights['vader']
            total_weight += weights['vader']
        
        # TextBlob ì ìˆ˜
        if 'textblob' in sentiment_scores:
            textblob_score = sentiment_scores['textblob']['polarity']
            final_score += textblob_score * weights['textblob']
            total_weight += weights['textblob']
        
        # ìŒì•… ì»¤ìŠ¤í…€ ì ìˆ˜
        if 'music_custom' in sentiment_scores:
            music_score = sentiment_scores['music_custom']['score']
            final_score += music_score * weights['music_custom']
            total_weight += weights['music_custom']
        
        # ê°€ì¤‘ í‰ê· 
        if total_weight > 0:
            final_score = final_score / total_weight
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = total_weight / sum(weights.values())
        
        return {
            'score': final_score,
            'confidence': confidence,
            'label': 'positive' if final_score > 0.1 else 'negative' if final_score < -0.1 else 'neutral'
        }
    
    def _categorize_emotion(self, sentiment: Dict) -> str:
        """ê°ì •ì„ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        score = sentiment['score']
        
        if score > 0.5:
            return 'very_positive'
        elif score > 0.1:
            return 'positive'
        elif score > -0.1:
            return 'neutral'
        elif score > -0.5:
            return 'negative'
        else:
            return 'very_negative'
    
    def _calculate_sentiment_trend(self, sentiment_results: List[Dict]) -> Dict:
        """ê°ì • ë³€í™” íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(sentiment_results) < 5:
            return {'trend': 'insufficient_data'}
        
        # ì‹œê°„ìˆœ ì •ë ¬ (timestampê°€ ìˆëŠ” ê²½ìš°)
        sorted_results = sorted(
            [r for r in sentiment_results if r.get('timestamp')],
            key=lambda x: x['timestamp']
        )
        
        if len(sorted_results) < 5:
            # timestampê°€ ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ ë¶„ì„
            sorted_results = sentiment_results
        
        # ì´ˆê¸° vs ìµœê·¼ ë¹„êµ
        first_half = sorted_results[:len(sorted_results)//2]
        second_half = sorted_results[len(sorted_results)//2:]
        
        first_avg = statistics.mean([r['final_sentiment']['score'] for r in first_half])
        second_avg = statistics.mean([r['final_sentiment']['score'] for r in second_half])
        
        change = second_avg - first_avg
        
        if change > 0.1:
            trend = 'improving'
        elif change < -0.1:
            trend = 'declining'
        else:
            trend = 'stable'
        
        return {
            'trend': trend,
            'change_magnitude': change,
            'early_sentiment': first_avg,
            'recent_sentiment': second_avg
        }
    
    def extract_comment_topics(self, comments: List[str], num_topics: int = 5) -> Dict:
        """
        ëŒ“ê¸€ì—ì„œ í† í”½ ì¶”ì¶œ (LDA ëª¨ë¸ë§)
        
        Args:
            comments: ëŒ“ê¸€ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            num_topics: ì¶”ì¶œí•  í† í”½ ìˆ˜
            
        Returns:
            í† í”½ ëª¨ë¸ë§ ê²°ê³¼
        """
        try:
            if not SKLEARN_AVAILABLE:
                return {'error': 'scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}
            
            if len(comments) < 10:
                return {'error': 'í† í”½ ë¶„ì„ì„ ìœ„í•´ ìµœì†Œ 10ê°œ ì´ìƒì˜ ëŒ“ê¸€ì´ í•„ìš”í•©ë‹ˆë‹¤'}
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_comments = []
            for comment in comments:
                # ê¸°ë³¸ ì „ì²˜ë¦¬
                clean_text = re.sub(r'[^\w\s]', ' ', comment.lower())
                clean_text = re.sub(r'\s+', ' ', clean_text).strip()
                
                if len(clean_text) > 10:  # ë„ˆë¬´ ì§§ì€ ëŒ“ê¸€ ì œì™¸
                    processed_comments.append(clean_text)
            
            if len(processed_comments) < 5:
                return {'error': 'ë¶„ì„ ê°€ëŠ¥í•œ ëŒ“ê¸€ì´ ë¶€ì¡±í•©ë‹ˆë‹¤'}
            
            # TF-IDF ë²¡í„°í™”
            try:
                tfidf_matrix = self.vectorizer.fit_transform(processed_comments)
                feature_names = self.vectorizer.get_feature_names_out()
            except Exception as e:
                return {'error': f'ë²¡í„°í™” ì˜¤ë¥˜: {str(e)}'}
            
            # LDA í† í”½ ëª¨ë¸ë§
            lda_model = LatentDirichletAllocation(
                n_components=min(num_topics, len(processed_comments)//2),
                random_state=42,
                max_iter=10
            )
            
            try:
                lda_model.fit(tfidf_matrix)
            except Exception as e:
                return {'error': f'LDA ëª¨ë¸ë§ ì˜¤ë¥˜: {str(e)}'}
            
            # í† í”½ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
            topics = []
            for topic_idx, topic in enumerate(lda_model.components_):
                # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
                top_keywords_idx = topic.argsort()[-10:][::-1]
                top_keywords = [feature_names[i] for i in top_keywords_idx]
                topic_weights = [topic[i] for i in top_keywords_idx]
                
                # í† í”½ ë¼ë²¨ ìƒì„±
                topic_label = self._generate_topic_label(top_keywords)
                
                topics.append({
                    'topic_id': topic_idx,
                    'label': topic_label,
                    'keywords': top_keywords,
                    'weights': topic_weights,
                    'music_relevance': self._calculate_music_relevance(top_keywords)
                })
            
            # ëŒ“ê¸€ë³„ í† í”½ ë¶„í¬
            doc_topic_dist = lda_model.transform(tfidf_matrix)
            
            # í† í”½ë³„ ëŒ“ê¸€ ìˆ˜ ê³„ì‚°
            topic_comment_counts = defaultdict(int)
            for doc_topics in doc_topic_dist:
                dominant_topic = doc_topics.argmax()
                topic_comment_counts[dominant_topic] += 1
            
            result = {
                'num_topics_found': len(topics),
                'topics': topics,
                'topic_distribution': dict(topic_comment_counts),
                'total_comments_analyzed': len(processed_comments),
                'model_perplexity': lda_model.perplexity(tfidf_matrix),
                'analyzed_at': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            self.console_log(f"[Comment] í† í”½ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e)}
    
    def _generate_topic_label(self, keywords: List[str]) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í† í”½ ë¼ë²¨ ìƒì„±"""
        # ìŒì•… ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„  ê²€ì‚¬
        for genre, indicators in self.genre_indicators.items():
            for indicator in indicators:
                if any(indicator.lower() in keyword.lower() for keyword in keywords[:3]):
                    return f"ìŒì•…_{genre}"
        
        # íŠ¸ë Œë“œ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì‚¬
        for trend_type, indicators in self.trend_indicators.items():
            for indicator in indicators:
                if any(indicator.lower() in keyword.lower() for keyword in keywords[:3]):
                    return f"íŠ¸ë Œë“œ_{trend_type}"
        
        # ì¼ë°˜ ë¼ë²¨ (ìƒìœ„ 2ê°œ í‚¤ì›Œë“œ ì¡°í•©)
        return f"{keywords[0]}_{keywords[1]}" if len(keywords) >= 2 else keywords[0]
    
    def _calculate_music_relevance(self, keywords: List[str]) -> float:
        """í‚¤ì›Œë“œì˜ ìŒì•… ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        music_related_count = 0
        
        # ëª¨ë“  ìŒì•… ê´€ë ¨ í‚¤ì›Œë“œì™€ ë¹„êµ
        all_music_keywords = []
        for genre_keywords in self.genre_indicators.values():
            all_music_keywords.extend(genre_keywords)
        for trend_keywords in self.trend_indicators.values():
            all_music_keywords.extend(trend_keywords)
        
        for keyword in keywords[:5]:  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ê²€ì‚¬
            for music_keyword in all_music_keywords:
                if music_keyword.lower() in keyword.lower():
                    music_related_count += 1
                    break
        
        return music_related_count / min(len(keywords), 5)
    
    def analyze_comment_patterns(self, comments: List[Dict]) -> Dict:
        """
        ëŒ“ê¸€ íŒ¨í„´ ë¶„ì„
        
        Args:
            comments: ëŒ“ê¸€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ëŒ“ê¸€ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        try:
            if not comments:
                return {'error': 'ë¶„ì„í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤'}
            
            # ê¸°ë³¸ í†µê³„
            comment_lengths = [len(comment.get('text', '')) for comment in comments]
            word_counts = [len(comment.get('text', '').split()) for comment in comments]
            
            # ì‹œê°„ íŒ¨í„´ ë¶„ì„
            time_patterns = self._analyze_time_patterns(comments)
            
            # ì–¸ì–´ íŒ¨í„´ ë¶„ì„
            language_patterns = self._analyze_language_patterns(comments)
            
            # ì°¸ì—¬ë„ íŒ¨í„´ ë¶„ì„
            engagement_patterns = self._analyze_engagement_patterns(comments)
            
            # ì´ëª¨ì§€/ì´ëª¨í‹°ì½˜ ë¶„ì„
            emoji_patterns = self._analyze_emoji_patterns(comments)
            
            result = {
                'basic_statistics': {
                    'total_comments': len(comments),
                    'avg_comment_length': statistics.mean(comment_lengths) if comment_lengths else 0,
                    'avg_word_count': statistics.mean(word_counts) if word_counts else 0,
                    'max_comment_length': max(comment_lengths) if comment_lengths else 0,
                    'min_comment_length': min(comment_lengths) if comment_lengths else 0
                },
                'time_patterns': time_patterns,
                'language_patterns': language_patterns,
                'engagement_patterns': engagement_patterns,
                'emoji_patterns': emoji_patterns,
                'analyzed_at': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            self.console_log(f"[Comment] ëŒ“ê¸€ íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_time_patterns(self, comments: List[Dict]) -> Dict:
        """ëŒ“ê¸€ ì‹œê°„ íŒ¨í„´ ë¶„ì„"""
        timestamps = []
        for comment in comments:
            timestamp = comment.get('timestamp')
            if timestamp:
                try:
                    if isinstance(timestamp, str):
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    else:
                        dt = timestamp
                    timestamps.append(dt)
                except:
                    continue
        
        if not timestamps:
            return {'error': 'ì‹œê°„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # ì‹œê°„ëŒ€ë³„ ë¶„í¬
        hour_distribution = defaultdict(int)
        day_distribution = defaultdict(int)
        
        for ts in timestamps:
            hour_distribution[ts.hour] += 1
            day_distribution[ts.strftime('%A')] += 1
        
        # í”¼í¬ ì‹œê°„ ì°¾ê¸°
        peak_hour = max(hour_distribution.items(), key=lambda x: x[1])[0] if hour_distribution else None
        peak_day = max(day_distribution.items(), key=lambda x: x[1])[0] if day_distribution else None
        
        return {
            'hour_distribution': dict(hour_distribution),
            'day_distribution': dict(day_distribution),
            'peak_hour': peak_hour,
            'peak_day': peak_day,
            'total_timespan_hours': (max(timestamps) - min(timestamps)).total_seconds() / 3600 if len(timestamps) > 1 else 0
        }
    
    def _analyze_language_patterns(self, comments: List[Dict]) -> Dict:
        """ì–¸ì–´ íŒ¨í„´ ë¶„ì„"""
        korean_count = 0
        english_count = 0
        mixed_count = 0
        emoji_count = 0
        
        for comment in comments:
            text = comment.get('text', '')
            
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            emoji_chars = len(re.findall(r'[ğŸ˜€-ğŸ™]', text))
            
            if emoji_chars > 0:
                emoji_count += 1
            
            if korean_chars > english_chars * 2:
                korean_count += 1
            elif english_chars > korean_chars * 2:
                english_count += 1
            else:
                mixed_count += 1
        
        total = len(comments)
        return {
            'korean_dominant': korean_count,
            'english_dominant': english_count,
            'mixed_language': mixed_count,
            'emoji_usage': emoji_count,
            'korean_percentage': (korean_count / total * 100) if total > 0 else 0,
            'english_percentage': (english_count / total * 100) if total > 0 else 0,
            'emoji_percentage': (emoji_count / total * 100) if total > 0 else 0
        }
    
    def _analyze_engagement_patterns(self, comments: List[Dict]) -> Dict:
        """ì°¸ì—¬ë„ íŒ¨í„´ ë¶„ì„"""
        scores = [comment.get('score', 0) for comment in comments if 'score' in comment]
        reply_counts = [comment.get('reply_count', 0) for comment in comments if 'reply_count' in comment]
        
        if not scores and not reply_counts:
            return {'error': 'ì°¸ì—¬ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        result = {}
        
        if scores:
            result['score_stats'] = {
                'avg_score': statistics.mean(scores),
                'max_score': max(scores),
                'min_score': min(scores),
                'high_score_comments': len([s for s in scores if s > 10])
            }
        
        if reply_counts:
            result['reply_stats'] = {
                'avg_replies': statistics.mean(reply_counts),
                'max_replies': max(reply_counts),
                'comments_with_replies': len([r for r in reply_counts if r > 0])
            }
        
        return result
    
    def _analyze_emoji_patterns(self, comments: List[Dict]) -> Dict:
        """ì´ëª¨ì§€/ì´ëª¨í‹°ì½˜ íŒ¨í„´ ë¶„ì„"""
        emoji_counter = Counter()
        emoticon_counter = Counter()
        
        # ì´ëª¨í‹°ì½˜ íŒ¨í„´ (ê°„ë‹¨í•œ)
        emoticon_patterns = [
            r':-?\)', r':-?\(', r':-?D', r':-?P', r':-?o', r':-?\|',
            r':\)', r':\(', r':D', r':P', r':o', r':\|',
            r'\^_\^', r'T_T', r'ã… ã… ', r'ã…‹ã…‹', r'ã…ã…'
        ]
        
        for comment in comments:
            text = comment.get('text', '')
            
            # ì´ëª¨ì§€ ì¹´ìš´íŠ¸
            emojis = re.findall(r'[ğŸ˜€-ğŸ™]', text)
            for emoji in emojis:
                emoji_counter[emoji] += 1
            
            # ì´ëª¨í‹°ì½˜ ì¹´ìš´íŠ¸
            for pattern in emoticon_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    emoticon_counter[match] += 1
        
        return {
            'top_emojis': dict(emoji_counter.most_common(10)),
            'top_emoticons': dict(emoticon_counter.most_common(10)),
            'total_emoji_usage': sum(emoji_counter.values()),
            'total_emoticon_usage': sum(emoticon_counter.values()),
            'emoji_diversity': len(emoji_counter),
            'emoticon_diversity': len(emoticon_counter)
        }
    
    def get_analysis_status(self) -> Dict:
        """ë¶„ì„ê¸° ìƒíƒœ í™•ì¸"""
        return {
            'sklearn_available': SKLEARN_AVAILABLE,
            'textblob_available': TEXTBLOB_AVAILABLE,
            'vader_available': VADER_AVAILABLE,
            'vader_initialized': self.vader_analyzer is not None,
            'vectorizer_ready': self.vectorizer is not None,
            'sentiment_categories': len(self.music_sentiment_keywords),
            'genre_indicators': len(self.genre_indicators),
            'trend_indicators': len(self.trend_indicators)
        }